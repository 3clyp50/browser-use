---
title: "Quickstart"
description: "Start using Browser Use with this quickstart guide"
icon: "rocket"
---

<Info>
  You can skip this steps by using [Browser Use Cloud](/cloud/quickstart)
</Info>

## Prepare the environment

Use [uv](https://docs.astral.sh/uv/) to setup the Python environment.

```bash
uv venv --python 3.12
```

and activate it with:

```bash
# For Mac/Linux:
source .venv/bin/activate

# For Windows:
.venv\Scripts\activate
```

Install the dependencies:

```bash
uv pip install browser-use
```

Then install playwright (or manually install the chromium browser):

```bash
uvx playwright install chromium --with-deps
```

## Create an agent

Then you can use the agent as follows:

```python agent.py
from browser_use.llm import ChatOpenAI
from browser_use import Agent
from dotenv import load_dotenv
load_dotenv()

import asyncio

llm = ChatOpenAI(model="gpt-5")

async def main():
    agent = Agent(
        task="Go to Hacker News and find the number 1 trending on Show HN",
        llm=llm,
    )
    result = await agent.run()
    print(result)

asyncio.run(main())
```

## Set up your LLM API keys

You need to set up API keys for the LLM you want to use and store them in `.env` file. For example, for OpenAI and Anthropic:

```bash .env
OPENAI_API_KEY=
ANTHROPIC_API_KEY=
```

For other LLM models you can refer to the [Supported Models](/customize/supported-models) page to find how to set them up with their specific API keys.

## Speed-Optimized Example

For maximum performance and speed, check out our speed-optimized example that combines all available optimizations:

```python speed_optimized.py
import asyncio
from dotenv import load_dotenv
load_dotenv()

from browser_use import Agent
from browser_use.llm import ChatGroq
from browser_use.browser.profile import BrowserProfile

# Speed optimization instructions for the model
SPEED_OPTIMIZATION_PROMPT = """
SPEED OPTIMIZATION INSTRUCTIONS:
- Be extremely concise and direct in your responses
- Skip unnecessary explanations and focus on actions
- Use multi-action sequences whenever possible to reduce steps
- Prioritize efficiency over detailed reasoning
- Get to the goal as quickly as possible
"""

async def main():
    # 1. Use fast LLM - Llama 4 on Groq for ultra-fast inference
    llm = ChatGroq(
        model='meta-llama/llama-4-maverick-17b-128e-instruct',
        temperature=0.0,  # Deterministic for speed
    )
    
    # 2. Create speed-optimized browser profile
    browser_profile = BrowserProfile(
        wait_between_actions=0.1,  # Reduced from default 0.5s
        minimum_wait_page_load_time=0.1,  # Reduced from default 0.25s
        wait_for_network_idle_page_load_time=0.25,  # Reduced from default 0.5s
        maximum_wait_page_load_time=3.0,  # Reduced from default 5.0s
        headless=True,  # Set to True for maximum speed, False for visibility
        disable_security=True,  # Skip security checks for speed
    )
    
    # 3. Create agent with all speed optimizations
    agent = Agent(
        task="Go to example.com and extract the main heading",
        llm=llm,
        browser_profile=browser_profile,
        flash_mode=True,  # Disables thinking and evaluation for maximum speed
        max_actions_per_step=10,  # Allow multiple actions per step
        extend_system_message=SPEED_OPTIMIZATION_PROMPT,  # Encourage concise responses
        vision_detail_level='low',  # Faster image processing
        max_failures=2,  # Reduce retry attempts
        retry_delay=5,  # Faster retry cycles
    )
    
    await agent.run()

asyncio.run(main())
```

**Speed Optimizations Included:**
- üöÄ **Flash Mode**: Disables thinking and evaluation steps
- ‚ö° **Fast LLM**: Llama 4 on Groq for ultra-fast inference  
- ‚è±Ô∏è **Reduced Wait Times**: 0.1s between actions (vs 0.5s default)
- üëÅÔ∏è **Low Vision Detail**: Faster image processing
- üéØ **Concise System Prompt**: Encourages direct, efficient responses
- üñ•Ô∏è **Headless Mode**: Optional for maximum rendering speed
- üîÑ **Multi-Actions**: Up to 10 actions per step for efficiency

**Environment Variables:**
```bash .env
GROQ_API_KEY=your_groq_api_key_here
```

This configuration can achieve 3-5x faster execution compared to default settings. For production environments where speed is critical, enable `headless=True` in the browser profile.
